{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0f761a",
   "metadata": {},
   "source": [
    "# WEEK 11\n",
    "\n",
    "2024/06/17 - 2024/06/23\n",
    "\n",
    "## 深度学习 C2_W3\n",
    "\n",
    "### 3.1 Softmax Classification\n",
    "\n",
    "- **软最大分类（Softmax Classification）**：\n",
    "  - 软最大函数（Softmax Function）是多类分类问题中的一种常用激活函数，它将输入向量转换为概率分布。\n",
    "  - 计算每个类别的概率，并选择概率最大的类别作为预测结果。\n",
    "  - 公式：\n",
    "    $$\n",
    "    \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "    $$\n",
    "  - 其中，$z_i$ 是输入向量 $z$ 中的第 $i$ 个元素，$\\sum_{j} e^{z_j}$ 是所有输入元素的指数和。\n",
    "\n",
    "#### 代码\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # 防止指数溢出\n",
    "    return exp_z / exp_z.sum(axis=0, keepdims=True)\n",
    "\n",
    "# 示例数据\n",
    "z = np.array([[2.0, 1.0, 0.1], [1.0, 2.0, 0.1], [0.1, 0.2, 0.3]])\n",
    "print(softmax(z))  # 输出：每个元素表示该类别的概率\n",
    "\n",
    "# 更复杂的示例，适用于批量数据\n",
    "batch_z = np.random.randn(5, 10)  # 生成随机数据，5个样本，每个样本10个特征\n",
    "softmax_probs = softmax(batch_z)\n",
    "print(softmax_probs)  # 输出：5x10的概率矩阵\n",
    "```\n",
    "\n",
    "- 在深度学习模型中，软最大函数通常作为输出层的激活函数，配合交叉熵损失函数使用。\n",
    "\n",
    "### 3.2 批量规范化\n",
    "#### 批量规范化（Batch Normalization）\n",
    "- 批量归一化是一种在神经网络中对输入数据进行标准化的方法。\n",
    "- 标准化后的数据可以更好地适应网络的激活函数，使得网络更容易学习和收敛。\n",
    "- 批量归一化可以帮助我们训练更深的网络，并且可以更快地收敛到更好的结果。\n",
    "- 批量归一化通过对每一层的输入进行归一化，使得它们的均值接近于0，方差接近于1，从而使得训练过程更加稳定。\n",
    "- 批量归一化还引入了一些额外的参数，用于控制归一化的过程，这些参数可以通过梯度下降等优化算法进行更新。\n",
    "\n",
    "\n",
    "  - 通过在每层网络中加入规范化操作，加速训练过程。\n",
    "  - 批量规范化的公式为：\n",
    "    $$\n",
    "    \\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "    $$\n",
    "  - 其中，$\\mu$ 是批量数据的均值，$\\sigma^2$ 是批量数据的方差，$\\epsilon$ 是一个很小的常数，防止除零。\n",
    "\n",
    "#### 代码\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# 构建模型\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),  # 应用批量规范化\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 加载数据\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 训练模型\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 评估模型\n",
    "model.evaluate(x_test, y_test)\n",
    "```\n",
    "\n",
    "- 批量规范化通过减小内部协变量偏移，使模型训练更稳定、收敛更快。\n",
    "\n",
    "### 3.3 在 TensorFlow 中构建神经网络\n",
    "\n",
    "- **TensorFlow 构建神经网络**：\n",
    "  - 使用 TensorFlow 构建和训练神经网络。\n",
    "  - 利用 TensorFlow 数据集进行训练和验证。\n",
    "\n",
    "#### 代码\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 构建模型\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 加载数据\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "```\n",
    "\n",
    "- 通过以上步骤，我们构建了一个包含两个隐藏层、使用批量规范化和Dropout正则化的神经网络，并在MNIST数据集上进行了训练和评估。\n",
    "\n",
    "### 3.4 GradientTape 的用途和操作方法\n",
    "\n",
    "- **GradientTape**：\n",
    "  - TensorFlow 中用于记录操作以计算梯度的工具。\n",
    "  - 常用于自定义训练循环。\n",
    "\n",
    "#### 代码\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定义模型参数\n",
    "W = tf.Variable(tf.random.normal((3, 2)), name='weight')\n",
    "b = tf.Variable(tf.zeros(2), name='bias')\n",
    "\n",
    "# 定义前向传播\n",
    "def model(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "# 定义损失函数\n",
    "def loss_fn(X, Y):\n",
    "    logits = model(X)\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(Y, logits))\n",
    "\n",
    "# 生成示例数据\n",
    "X = tf.random.normal((100, 3))\n",
    "Y = tf.random.uniform((100, 2), maxval=2, dtype=tf.int32)\n",
    "Y = tf.one_hot(Y, depth=2)\n",
    "\n",
    "# 使用 GradientTape 计算梯度\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = loss_fn(X, Y)\n",
    "gradients = tape.gradient(loss, [W, b])\n",
    "print(f\"Gradients: {gradients}\")\n",
    "```\n",
    "\n",
    "- GradientTape 使得自定义训练过程变得更加灵活，可以手动控制前向传播、计算损失和反向传播。\n",
    "\n",
    "### 3.5 使用 tf.Variable\n",
    "\n",
    "- **tf.Variable**：\n",
    "  - TensorFlow 中的可变对象，用于存储模型参数，可以在训练过程中更新。\n",
    "\n",
    "#### 代码\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定义变量\n",
    "a = tf.Variable(1.0)\n",
    "b = tf.Variable(2.0)\n",
    "\n",
    "# 使用 GradientTape 计算梯度并更新变量\n",
    "with tf.GradientTape() as tape:\n",
    "    c = a * b\n",
    "gradients = tape.gradient(c, [a, b])\n",
    "print(f\"Gradients: {gradients}\")\n",
    "\n",
    "# 更新变量\n",
    "learning_rate = 0.1\n",
    "a.assign_sub(learning_rate * gradients[0])\n",
    "b.assign_sub(learning_rate * gradients[1])\n",
    "print(f\"Updated a: {a.numpy()}, Updated b: {b.numpy()}\")\n",
    "```\n",
    "\n",
    "### 3.6 应用 TensorFlow 装饰器加快代码速度\n",
    "\n",
    "- **TensorFlow 装饰器**：\n",
    "  - 使用 @tf.function 装饰器来提升代码执行效率，将 Python 函数编译成高效的 TensorFlow 图。\n",
    "\n",
    "#### 代码\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def compute_sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "result = compute_sum(tf.constant(1), tf.constant(2))\n",
    "print(f\"Result: {result}\")  \n",
    "\n",
    "@tf.function\n",
    "def training_step(model, optimizer, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, predictions))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(10, activation='softmax')])\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "inputs = tf.random.normal((32, 784))\n",
    "targets = tf.random.uniform((32,), maxval=10, dtype=tf.int32)\n",
    "loss = training_step(model, optimizer, inputs, targets)\n",
    "print(f\"Training loss: {loss.numpy()}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 深度学习 C3_W1\n",
    "\n",
    "### 1.1 机器学习战略\n",
    "\n",
    "- **机器学习战略**：\n",
    "  - 制定明确的战略对于机器学习项目的成功至关重要。\n",
    "  - 战略可以帮助团队集中精力，优先处理关键问题，提高模型性能。\n",
    "\n",
    "#### 解释\n",
    "  - **确定目标**：机器学习项目应该有明确的目标，例如提高准确率、减少错误率等。\n",
    "  - **资源分配**：根据项目的优先级和目标，合理分配资源，包括时间、人员和计算资源。\n",
    "  - **持续评估**：在项目的每个阶段，评估模型的表现，及时调整策略。\n",
    "\n",
    "### 1.2 为 ML 项目设定目标\n",
    "\n",
    "- **满意度和优化指标**：\n",
    "  - 使用这些指标来评估模型性能，设定项目目标。\n",
    "  - 例如：准确率、精确率、召回率、F1 分数等。\n",
    "\n",
    "#### 解释\n",
    "  - **满意度指标**：反映用户对模型的满意程度，如用户满意度评分。\n",
    "  - **优化指标**：用于优化模型性能的指标，如准确率、精确率、召回率和 F1 分数。\n",
    "\n",
    "### 1.3 为数据集选择正确的训练/验证/测试分集\n",
    "\n",
    "- **数据集划分**：\n",
    "  - 合理划分训练集、验证集和测试集，确保模型的泛化能力。\n",
    "  - 常见的比例：训练集 70%，验证集 15%，测试集 15%。\n",
    "\n",
    "#### 解释\n",
    "  - **训练集**：用于训练模型，调整参数。\n",
    "  - **验证集**：用于调参和模型选择，避免过拟合。\n",
    "  - **测试集**：用于评估模型的最终性能，确保模型能够泛化到未见过的数据。\n",
    "\n",
    "### 1.4 定义人类水平的性能\n",
    "\n",
    "- **人类水平性能**：\n",
    "  - 将人类的表现作为基准，用于衡量模型性能。\n",
    "  - 例如：在医学图像诊断中，医生的准确率可以作为基准。\n",
    "\n",
    "#### 解释\n",
    "  - **基准设定**：通过对比模型和人类的性能，设定性能基准。\n",
    "  - **性能评估**：将模型性能与人类水平进行对比，找出改进方向。\n",
    "\n",
    "### 1.5 人类水平\n",
    "\n",
    "- **关键优先事项**：\n",
    "  - 通过比较模型和人类的性能，确定项目的优先改进方向。\n",
    "  - 例如：如果模型性能已经接近人类水平，可以尝试提高数据质量或增加数据量。\n",
    "\n",
    "#### 解释\n",
    "  - **确定改进点**：通过分析模型与人类表现的差距，找出模型需要改进的具体方面。\n",
    "  - **优先级排序**：根据改进点的重要性和可行性，排序优先级，集中资源进行优化。\n",
    "\n",
    "### 1.6 ML 战略决策\n",
    "\n",
    "- **战略决策**：\n",
    "  - 基于性能和数据集的观察，调整和优化机器学习战略。\n",
    "  - 例如：如果模型在特定类型的数据上表现较差，可以针对性地收集更多该类型的数据进行训练。\n",
    "\n",
    "#### 解释\n",
    "  - **数据观察**：通过观察数据分布和模型在不同数据上的表现，找出问题所在。\n",
    "  - **调整策略**：根据观察结果，调整数据收集和模型训练策略，提升整体性能。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88e084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
