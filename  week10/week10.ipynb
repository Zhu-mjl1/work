{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da39ae2",
   "metadata": {},
   "source": [
    "# WEEK 10\n",
    "\n",
    "2024/06/10 - 2024/06/16\n",
    "\n",
    "# 深度学习 C2_W1\n",
    "\n",
    "## 1.1 初始化方法的影响\n",
    "\n",
    "- **零初始化**：\n",
    "  - 将所有权重初始化为零。\n",
    "  - 问题：对称性破坏，每个神经元的输出相同，无法学习不同特征。\n",
    "  \n",
    "- **随机初始化**：\n",
    "  - 使用小随机数初始化权重，避免对称性。\n",
    "  - 适用情况：可以使用均匀分布或正态分布。\n",
    "\n",
    "- **He 初始化**：\n",
    "  - 适用于 ReLU 激活函数。\n",
    "  - 公式： $ W = \\sqrt{\\frac{2}{n}} \\cdot \\text{randn}(n, m) $\n",
    "\n",
    "### 代码\n",
    "\n",
    "```python\n",
    "def initialize_parameters_he(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)           \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n",
    "```\n",
    "\n",
    "## 1.2 数据集的划分\n",
    "\n",
    "- **训练集**：用于训练模型，调整权重参数。\n",
    "- **验证集**：用于调参和模型选择，不参与训练。\n",
    "- **测试集**：用于评估模型的最终性能，不参与训练和验证。\n",
    "\n",
    "## 1.3 诊断偏差和方差\n",
    "\n",
    "- **高偏差**（欠拟合）：\n",
    "  - 训练误差和验证误差都高。\n",
    "  - 解决方法：增加模型复杂度，训练更长时间，选择更好的特征。\n",
    "\n",
    "- **高方差**（过拟合）：\n",
    "  - 训练误差低，但验证误差高。\n",
    "  - 解决方法：增加训练数据，使用正则化，选择更简单的模型。\n",
    "\n",
    "## 1.4 正则化方法\n",
    "\n",
    "- **L2 正则化**：\n",
    "  - 在损失函数中加入权重平方和的惩罚项。\n",
    "  - 公式： $J_{reg} = J + \\frac{\\lambda}{2m} \\sum W^2 $\n",
    "\n",
    "- **Dropout**：\n",
    "  - 在训练过程中随机忽略一部分神经元。\n",
    "  - 优点：减少过拟合，增强模型的泛化能力。\n",
    "  \n",
    "### 代码\n",
    "\n",
    "```python\n",
    "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(A3, Y)\n",
    "    \n",
    "    L2_regularization_cost = (1/m) * (lambd/2) * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost\n",
    "```\n",
    "\n",
    "## 1.5 消失梯度和爆炸梯度\n",
    "\n",
    "- **消失梯度**：梯度在反向传播过程中逐渐变小，导致权重更新缓慢。\n",
    "- **爆炸梯度**：梯度在反向传播过程中逐渐变大，导致权重更新过大。\n",
    "- 解决方法：\n",
    "  - **He 初始化**：使用较大的初始值。\n",
    "  - **正则化**：L2 正则化和 Dropout。\n",
    "  - **梯度裁剪**：限制梯度的最大值。\n",
    "\n",
    "## 1.6 梯度检查\n",
    "\n",
    "- **梯度检查**：通过数值方法计算梯度，验证反向传播的正确性。\n",
    "- 实现步骤：\n",
    "  1. 计算数值梯度： $ \\frac{\\partial J}{\\partial \\theta} \\approx \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon} $\n",
    "  2. 计算反向传播的梯度。\n",
    "  3. 比较数值梯度和反向传播的梯度。\n",
    "\n",
    "### 代码\n",
    "\n",
    "```python\n",
    "def gradient_check(parameters, gradients, X, Y, epsilon=1e-7):\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    for i in range(num_parameters):\n",
    "        thetaplus = np.copy(parameters_values)\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon\n",
    "        J_plus[i] = compute_cost(from_vector_to_parameters(thetaplus), X, Y)\n",
    "        \n",
    "        thetaminus = np.copy(parameters_values)\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon\n",
    "        J_minus[i] = compute_cost(from_vector_to_parameters(thetaminus), X, Y)\n",
    "        \n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "    \n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    return difference\n",
    "```\n",
    "\n",
    "## 1.7 初始化方法的应用\n",
    "\n",
    "- **零初始化**：权重初始化为零，不推荐。\n",
    "- **随机初始化**：小随机数初始化权重。\n",
    "- **He 初始化**：推荐用于 ReLU 激活函数的初始化方法。\n",
    "\n",
    "## 1.8 正则化的应用\n",
    "\n",
    "- 在模型训练过程中加入正则化项，可以减少过拟合，提高模型的泛化能力。\n",
    "- 正则化方法包括 L2 正则化和 Dropout 正则化。\n",
    "\n",
    "# 深度学习 C2_W2\n",
    "\n",
    "## 2.1 优化方法\n",
    "\n",
    "- **随机梯度下降（SGD）**：\n",
    "  - 每次迭代更新一个样本的梯度。\n",
    "  - 优点：计算效率高。\n",
    "  - 缺点：收敛速度慢，可能陷入局部最优。\n",
    "\n",
    "- **动量（Momentum）**：\n",
    "  - 引入动量项，加速收敛。\n",
    "  - 公式： $ v = \\beta v + (1 - \\beta) \\nabla J $\n",
    "  - 更新： $ \\theta = \\theta - \\alpha v $\n",
    "\n",
    "- **RMSProp**：\n",
    "  - 使用指数加权平均的方法，对梯度进行缩放。\n",
    "  - 公式： $ S = \\beta S + (1 - \\beta) \\nabla J^2 $\n",
    "  - 更新： $ \\theta = \\theta - \\alpha \\frac{\\nabla J}{\\sqrt{S + \\epsilon}}$\n",
    "\n",
    "- **Adam**：\n",
    "  - 结合动量和 RMSProp，具有自适应学习率。\n",
    "  - 公式：\n",
    "    - $ v = \\beta_1 v + (1 - \\beta_1) \\nabla J $\n",
    "    - $ s = \\beta_2 s + (1 - \\beta_2) \\nabla J^2$\n",
    "  - 更新：$ \\theta = \\theta - \\alpha \\frac{v}{\\sqrt{s + \\epsilon}}$\n",
    "\n",
    "### 代码\n",
    "\n",
    "```python\n",
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dW\" + str(l)] = beta * v[\"dW\" + str(l)] + (1 - beta) * grads[\"dW\" + str(l)]\n",
    "        v[\"db\" + str(l)] = beta * v[\"db\" + str(l)] + (1 - beta) * grads[\"db\" + str(l)]\n",
    "        \n",
    "        parameters[\"W\" + str(l)] -= learning_rate * v[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] -= learning_rate * v[\"db\" + str(l)]\n",
    "        \n",
    "    return parameters, v\n",
    "```\n",
    "\n",
    "## 2.2 随机小批量\n",
    "\n",
    "- **随机小批量（Mini-batch）**：\n",
    "  - 将训练数据划分为多个小批量，每次迭代使用一个小批量进行梯度更新。\n",
    "  - 优点：可以加速收敛，提高优化效果。\n",
    "\n",
    "### 代码\n",
    "\n",
    "```python\n",
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch\n",
    "\n",
    "_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "```\n",
    "\n",
    "## 2.3 学习率衰减\n",
    "\n",
    "- **学习率衰减**：\n",
    "  - 随着训练过程的进行，逐渐降低学习率。\n",
    "  - 益处：在训练初期以较大学习率快速收敛，在训练后期以较小学习率精细调整，防止过大的参数更新。\n",
    "\n",
    "- 常见的学习率衰减方法：\n",
    "  - **指数衰减**： $ \\alpha_t = \\alpha_0 \\cdot e^{-\\lambda t} $\n",
    "  - **分段常数衰减**：每经过一定的迭代次数，将学习率乘以一个常数。\n",
    "  - **1/t 衰减**：$ \\alpha_t = \\frac{\\alpha_0}{  1 + \\lambda t} $\n",
    "\n",
    "### 代码\n",
    "\n",
    "```python\n",
    "def update_parameters_with_decay(parameters, grads, learning_rate, decay_rate, iteration):\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    learning_rate = learning_rate / (1 + decay_rate * iteration)\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
    "        \n",
    "    return parameters\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
