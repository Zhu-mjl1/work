{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01434ce2",
   "metadata": {},
   "source": [
    "# week7\n",
    "*time: 2024-05-20 to 2024-05-26*\n",
    "\n",
    "## S3_W2 推荐学习\n",
    "### 2.1 协同过滤算法\n",
    "\n",
    "* 目标: 学习多个用户参数和物品特征，以便预测用户对未评分物品的评分。\n",
    "* 用户参数:\n",
    "    * 表示用户对不同特征的偏好\n",
    "* 物品特征:\n",
    "    * 表示物品在不同特征上的表现。\n",
    "    \n",
    "* 通过最小化成本函数，可以使用梯度下降等优化算法来学习用户参数和物品特征。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ad8ff",
   "metadata": {},
   "source": [
    "### 2.2 tensorflow 实现协同过滤算法\n",
    "```python\n",
    "w = tf.Variable(3.0)\n",
    "x = 1.0\n",
    "y = 1.0\n",
    "alpha = 0.01\n",
    "iterations = 30\n",
    "for iter in range(iterations):\n",
    "    with tf.GradientType() as tape:\n",
    "        fwb = w * x\n",
    "        costJ = (fwb - y)**2\n",
    "    [dJdw] = tape.gradient( costJ, [w] )\n",
    "    w.assign_add(-alpha * dJdw)\n",
    "\n",
    "```\n",
    "\n",
    "* 用上优化器：\n",
    "```python\n",
    "optimizer = keras.optimizer.Adam(learning_rate = 1e-1)\n",
    "\n",
    "iterations = 200\n",
    "for iter in range (iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost_value = cofiCodtFuncV(X, W, b, Ynorm, R, num_users, num_movies, lambda)\n",
    "        \n",
    "    grads = tape.gradient( cost_value, [X,W,b] )\n",
    "    optimizer.apply_gradients( zip(grads, [X,W,b]))\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1201f6b",
   "metadata": {},
   "source": [
    "### 2.3 基于内容的过滤\n",
    "* 协同过滤：\n",
    "    * 这种方法是基于用户之间的相似性来进行推荐。\n",
    "    * 它会分析用户的评分和偏好，然后找到与他们相似的其他用户，并根据这些相似用户的评分来推荐物品。\n",
    "    * 例如，如果用户A和用户B都给某个电影高分，那么协同过滤算法可能会将这个电影推荐给用户A，因为用户B和用户A有相似的品味。\n",
    "\n",
    "* 基于内容的过滤：\n",
    "    * 这种方法是基于物品或内容的特征来进行推荐。\n",
    "    * 它会分析物品的特征，例如电影的类型、演员等，以及用户的特征，例如年龄、性别等，然后根据这些特征来匹配用户和物品。\n",
    "    * 例如，如果用户喜欢动作片和科幻片，基于内容的过滤算法可能会推荐给他们具有这些特征的电影。\n",
    "    \n",
    "    \n",
    "### 2.4 使用深度学习构建基于内容的过滤算法\n",
    "* 我们使用神经网络来计算描述用户和电影的向量$v_u$和$v_m$。\n",
    "* 用户网络和电影网络可以具有不同的隐藏层和隐藏单元数，但输出层的维度必须相同。\n",
    "* 我们使用梯度下降或其他优化算法来训练神经网络的参数，使得预测的误差最小化。\n",
    "* 这个模型可以用来预测用户对电影的评分，也可以用来找到与给定电影相似的其他电影。\n",
    "* 我们可以预先计算出与每部电影相似的电影，以便在用户浏览特定电影时提供给他们。\n",
    "\n",
    "\n",
    "### 2.5 推荐系统\n",
    "* 推荐系统通常采用两个步骤：检索和排序。\n",
    "    * 检索：\n",
    "        * 在这一步骤中，系统会生成一个包含大量可能的推荐项目的列表。\n",
    "        * 这个列表会尽可能地覆盖用户可能喜欢的项目，因此可能会包含一些用户不太可能喜欢的项目。\n",
    "        * 检索步骤可以快速完成，得到一个包含100个或更多可能的推荐项目的列表。\n",
    "\n",
    "    * 排序：\n",
    "        * 在这一步骤中，系统会使用学习模型对检索步骤得到的列表进行排序。\n",
    "        * 系统会将用户特征向量和项目特征向量输入到神经网络中，计算出每个用户-项目对的预测评分。\n",
    "        * 根据这些评分，系统会选择用户最有可能给出高评分的项目，并将它们按照排名展示给用户。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7353d88",
   "metadata": {},
   "source": [
    "## S3_W3 强化学习\n",
    "### 3.1 强化学习简介\n",
    "* 概念：\n",
    "    * 强化学习是一种机器学习方法，它通过智能体与环境的交互来学习如何做出最优决策。\n",
    "    * 在强化学习中，智能体通过观察环境的状态，并根据奖励信号来选择行动。\n",
    "    * 奖励信号告诉智能体它在特定状态下的表现好坏，智能体的目标是通过最大化累积奖励来学习最优策略。\n",
    "    * 强化学习的关键是找到一个平衡探索和利用的策略(aka policy or $\\pi$)，以便在不断尝试新行动的同时，最大化累积奖励。\n",
    "* 应用领域：机器人控制、游戏玩法、金融交易等。\n",
    "* 强化学习的回报：\n",
    "    * 回报是强化学习中衡量动作序列好坏程度的指标。\n",
    "    * 回报通过对每个动作的奖励进行加权求和来计算。\n",
    "    * 加权使用折扣因子，折扣因子衡量未来奖励的重要性。\n",
    "    * 较早获得奖励的动作将得到较高的回报值。\n",
    "    * 回报的计算方式可以比较不同奖励序列的优劣。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530176d",
   "metadata": {},
   "source": [
    " **强化学习相关的概念总结：**\n",
    " \n",
    "|  序号  | 概念     | 解释 |\n",
    "|:----:|:-----|:-----|\n",
    "|   1    | 状态 states    |描述环境的情况或条件，如机器人的位置或传感器读数|\n",
    "|   2    |  动作 actions    |智能体在给定状态下可以执行的操作或决策，如向左移动或向右移动|\n",
    "|   3    |  奖励 rewards   |智能体在执行动作后从环境中获得的反馈信号，可以是正数、负数或零|\n",
    "|   4    |   折扣因子 discount factor $ \\gamma$  |衡量未来奖励的重要性，介于0和1之间的值|\n",
    "|   5    |    回报 return |智能体在一系列动作中获得的累积奖励，通过将未来的奖励与折扣因子相乘并加总得到|\n",
    "|   6    |    策略 policy $\\pi$ |智能体根据当前状态选择动作的方式或规则，可以是确定性的（给定状态选择一个确定的动作）或随机的（给定状态选择一个动作的概率分布|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9822318",
   "metadata": {},
   "source": [
    "### 3.2 状态-动作值函数（state-action value function）\n",
    "* 状态-动作值函数（Q值函数）是强化学习中的重要概念，用于衡量在特定状态下采取某个动作的价值。\n",
    "* Q值函数可以帮助我们选择最优的动作，最大化回报。\n",
    "* 通过计算Q值函数，我们可以确定在特定状态下采取哪个动作是最好的。\n",
    "* 强化学习算法可以用来计算Q值函数，并帮助我们找到最优策略。\n",
    "* 强化学习在许多实际问题中都有应用，如机器人控制、游戏策略和自动驾驶等。\n",
    "* 学习强化学习可以帮助我们了解如何使用Q值函数来制定最优策略，并不断优化以获得更好的结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba2bfd",
   "metadata": {},
   "source": [
    "### 3.3 连续状态空间\n",
    "\n",
    "* 连续状态空间是指在机器学习和强化学习中，状态可以采用任意连续值的情况。\n",
    "* 与离散状态空间不同，离散状态空间只能采用有限的几个可能值。\n",
    "* 在连续状态空间中，状态需要使用向量来表示，其中每个元素都可以取任意值。\n",
    "* 连续状态空间使问题更加复杂，因为需要处理大量的可能状态。\n",
    "\n",
    "### 3.4 DQN算法\n",
    "* 在之前的神经网络架构中，需要分别进行四次推理来计算每个动作的 Q 值，这在效率上是不高的。\n",
    "* 这个改进的架构可以同时输出所有四个动作的 Q 值，从而提高了算法的效率。\n",
    "\n",
    "* 具体地：\n",
    "\n",
    "    * 输入层：输入是代表状态的八个数字，表示着月球着陆器的状态。\n",
    "    * 隐藏层：有两个隐藏层，分别有 64 个神经元。\n",
    "    * 输出层：输出层有四个神经元，分别对应四个动作：不动、向左、向右、主引擎喷射。\n",
    "\n",
    "* 神经网络的任务是同时计算出给定状态下的所有四个动作的 Q 值。\n",
    "\n",
    "* 这种改进的架构的优势在于，给定一个状态，我们只需要进行一次推理就可以得到所有四个动作的 Q 值，然后快速选择具有最大 Q 值的动作。\n",
    "* 此外，这个改进的神经网络架构还使得计算 Bellman 方程中的 max 操作更加高效。\n",
    "* 我们可以同时获得所有可能动作的 Q 值，然后选择最大值来计算 Bellman 方程的右侧值。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d2e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
